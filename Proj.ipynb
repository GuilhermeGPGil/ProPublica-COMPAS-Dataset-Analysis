{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Project\n",
    "### Guilherme Gil, Alva Ringi & Francisco Simões\n",
    "\n",
    "## Dataset\n",
    "\n",
    "You will examine the ProPublica COMPAS dataset, which consists of all criminal defendants who were subject to COMPAS screening in Broward County, Florida, during 2013 and 2014. For each defendant, various information fields (‘features’) were also gathered by ProPublica. Broadly, these fields are related to the defendant’s demographic information (e.g., gender and race), criminal history (e.g., the number of prior offenses) and administrative information about the case (e.g., the case number, arrest date, risk of recidivism predicted by the COMPAS tool). Finally, the dataset also contains information about whether the defendant did actually recidivate or not.\n",
    "\n",
    "The COMPAS score uses answers to 137 questions to assign a risk score to defendants -- essentially a probability of re-arrest. The actual output is two-fold: a risk rating of 1-10 and a \"low\", \"medium\", or \"high\" risk label.\n",
    "\n",
    "Link to dataset: https://github.com/propublica/compas-analysis\n",
    "\n",
    "The file we will analyze is: compas-scores-two-years.csv\n",
    "\n",
    "Link to the ProPublica article:\n",
    "\n",
    "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\n",
    "\n",
    "\n",
    "## Project goal\n",
    "\n",
    "The project has three parts: \n",
    "\n",
    "1.  The COMPAS scores have been shown to have biases against certain racial groups. Analyze the dataset to highlight these biases.  \n",
    "\n",
    "2. Based on the features in the COMPAS dataset, train classifiers to predict who will re-offend (hint: no need to use all features, just the ones you find relevant).  Study if your classifiers are more or less fair than the COMPAS classifier. \n",
    "\n",
    "3. Build a fair classifier (last lecture will cover fair classification techniques). Is excluding the race from the feature set enough?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installations that might be needed to run this notebook\n",
    "\n",
    "# !pip3 install scikit-lego[all]\n",
    "# !pip3 install tensorflow\n",
    "# !pip3 install seaborn\n",
    "# !pip3 install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "We first need to load the data from the ProPublica repo:\n",
    "https://github.com/propublica/compas-analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "from random import seed, shuffle\n",
    "#from __future__ import division\n",
    "#from collections import defaultdict\n",
    "#import utils as ut\n",
    "\n",
    "SEED = 1234\n",
    "seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "pd.set_option('display.max_columns', None)\n",
    "def check_data_file(fname):\n",
    "    files = os.listdir(\".\") # get the current directory listing\n",
    "    print(\"Looking for file '%s' in the current directory...\",fname)\n",
    "\n",
    "    if fname not in files:\n",
    "        print(\"'%s' not found! Downloading from GitHub...\",fname)\n",
    "        addr = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
    "        response = urllib.request.urlopen(addr)\n",
    "        data = response.read()\n",
    "        fileOut = open(fname, \"wb\")\n",
    "        fileOut.write(data)\n",
    "        fileOut.close()\n",
    "        print(\"'%s' download and saved locally..\",fname)\n",
    "    else:\n",
    "        print(\"File found in current directory..\")\n",
    "    \n",
    "COMPAS_INPUT_FILE = \"compas-scores-two-years.csv\"\n",
    "check_data_file(COMPAS_INPUT_FILE)\n",
    "\n",
    "df = pd.read_csv(COMPAS_INPUT_FILE) # representing data as a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Analysis of the dataset and its biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The dataset contains data on how many convicts?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are \", df.shape[0], \"samples in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What are the features the dataset contains?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "\n",
    "\n",
    "- **Are there missing values (NaN)? are there outliers?**  \n",
    "\n",
    "hint pandas: check isnull function in pandas\n",
    "\n",
    "- **Does ProPublica mentions how to clean the data?**  \n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.isna(df[\"sex\"])\n",
    "# df[\"age\"].max()\n",
    "# df = df.dropna(axis=0, how='any', thresh=5, subset=[\"days_b_screening_arrest\"], inplace=False)\n",
    "# df = df.dropna(subset=[\"sex\", \"days_b_screening_arrest\"])\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "    \n",
    "- **What is the effect of the following function?**\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df = df.dropna(subset=[\"days_b_screening_arrest\"]) # dropping missing vals\n",
    "df = df[ (df.days_b_screening_arrest <= 30) &\n",
    "(df.days_b_screening_arrest >= -30) &\n",
    "(df.is_recid != -1) & (df.c_charge_degree != 'O') & (df.score_text != 'N/A') ]\n",
    "df.reset_index(inplace=True, drop=True) # renumber the rows from 0 again\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "\n",
    "**Comment**: It first removes all rows in the dataset that have no value for the feature \"days before screening arrest\". Then it filters out rows that does not full fill the given requirements. This can be used to filter out missing data and outliers. \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic analysis of demographics\n",
    "\n",
    "- **What are the different races present in the dataset?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"race\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What is the number of people by age category?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_cat_names = list(df[\"age_cat\"].unique().tolist())\n",
    "age_cat_size = list(df.groupby([\"age_cat\"], sort=False).size())\n",
    "plt.bar(age_cat_names, age_cat_size)\n",
    "plt.title(\"Number of people per age category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: There is a majority of people in the dataset are between 25 and 45 years old."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What is the number of people by COMPAS score (decile_score)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decile_score_names = list(df[\"decile_score\"].unique().tolist())\n",
    "decile_score_size = list(df.groupby(\"decile_score\", sort=False).size())\n",
    "plt.bar(decile_score_names, decile_score_size)\n",
    "plt.title(\"Number of people per COMPAS score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: The by far most common score is 1. The higher score, the less common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What is the number of people by COMPAS risk category (score_text)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_text_names = list(df[\"score_text\"].unique().tolist())\n",
    "score_text_size = list(df.groupby(\"score_text\", sort=False).size())\n",
    "plt.bar(score_text_names, score_text_size)\n",
    "plt.title(\"Number of people per COMPAS risk category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: The by far most common class is \"low score\", followed by \"medium\" and \"high\". This follows well the distribution of the numerical scores observed in the plot above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What is the number of people by race?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_names = list(df[\"race\"].unique().tolist())\n",
    "race_size = list(df.groupby(\"race\", sort=False).size())\n",
    "plt.bar(race_names, race_size)\n",
    "plt.xticks(rotation=20)\n",
    "plt.title(\"Number of people per race\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: There are very few Native American and Asian people in this dataset. We do not think there are enough people in these groups to have an unbiased estimator for them. For this reason, we will remove the categories \"Native American\" and \"Asian\" and place these samples in the category \"Other\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df.race == 'Asian'),'race']='Other'\n",
    "df.loc[(df.race == 'Native American'),'race']='Other'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic investigations of gender and race bias in COMPAS scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What is the mean COMPAS score (decile_score) per race and gender?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean score per race\n",
    "race_names = list(df[\"race\"].unique().tolist())\n",
    "race_score_mean = list(df.groupby([\"race\"], sort=False).mean()[\"decile_score\"])\n",
    "plt.bar(race_names, race_score_mean)\n",
    "plt.title(\"Mean COMPAS score per race\")\n",
    "plt.show()\n",
    "\n",
    "# mean score per gender\n",
    "sex_names = list(df[\"sex\"].unique().tolist())\n",
    "sex_score_mean = list(df.groupby([\"sex\"], sort=False).mean()[\"decile_score\"])\n",
    "plt.bar(sex_names, sex_score_mean)\n",
    "plt.title(\"Mean COMPAS score per gender\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: The mean score is the highest for the race group \"African-American\" which indicates that there is a bias against them. For men women, the mean score is similar which indicates that there are not much bias based on gender. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What is the distribution (histogram) of decile_score per race and gender?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decile score per race\n",
    "df.hist(column='decile_score', by=[\"race\"],figsize = (9,7));\n",
    "plt.show()\n",
    "\n",
    "# decile score per gender\n",
    "df.hist(column='decile_score', by=[\"sex\"],figsize = (9,3));\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: There seems to be a bias towards African American people since the histogram distribution on this race is more evenly distributed when compared to the other races histograms (we can observe a higher percentage of higher scores in the African American race). For the other races we can see that the  histogram is more squewed to lower values.\n",
    "\n",
    "Comparing men and women, we see that the decile score distribution is similar for the two genders which indicates that there is not much bias against any of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **How many people were re-arrested?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rearrested = df[df[\"two_year_recid\"]==1]\n",
    "print(\"Number of Re-arrests:\", len(rearrested.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Compute the recidivism (i.e., people that got re-arrested) rates by race and gender**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate number of re-arrested people as well as total number of people per race\n",
    "races_names = list(df[\"race\"].unique().tolist())\n",
    "race_nr_ra = list(df.loc[(df['two_year_recid'] == 0)].groupby([\"race\"], sort=False).size())\n",
    "race_nr = list(df.groupby([\"race\"], sort=False).size())\n",
    "\n",
    "# calculate ratios per race\n",
    "race_ra_rate = []\n",
    "for i in range(len(races_names)):\n",
    "    race_ra_rate.append(1-(race_nr_ra[i] / race_nr[i]))\n",
    "\n",
    "# plot result\n",
    "plt.bar(race_names, race_ra_rate)\n",
    "plt.title(\"Recidivism rate per race\")\n",
    "plt.show()\n",
    "\n",
    "# calculate number of re-arrested people as well as total number of people per gender\n",
    "sex_names = list(df[\"sex\"].unique().tolist())\n",
    "sex_nr_ra = list(df.loc[(df['two_year_recid'] == 0)].groupby([\"sex\"], sort=False).size())\n",
    "sex_nr = list(df.groupby([\"sex\"], sort=False).size())\n",
    "\n",
    "# calculate ratios per gender\n",
    "sex_ra_rate = []\n",
    "for i in range(len(sex_names)):\n",
    "    sex_ra_rate.append(1-(sex_nr_ra[i] / sex_nr[i]))\n",
    "\n",
    "# plot result    \n",
    "plt.bar(sex_names, sex_ra_rate)\n",
    "plt.title(\"Recidivism rate per sex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: The recidivism rate is higher for African-Americans compared to the other races. This indicates that the dataset itself is bias. With a bias dataset, classifiers will learn these patterns and, in this case, give higher decile scores to African-Americans.\n",
    "\n",
    "Comparing genders, the recidivism rate is higher for men than women. This is not as clearly reflected in the decile scores as in the case with race. The mean decile score for men and women is more equal than the recidivism rate in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr = df[[\"decile_score\",\"two_year_recid\",\"sex\",\"race\"]]\n",
    "\n",
    "# transform decile score [0, 10] to binary by splitting at decile score = 5\n",
    "def is_recid(row):\n",
    "    if(row[\"decile_score\"] >= 5):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "dfr['pred_is_recid'] = dfr.apply (lambda row: is_recid(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: We consider any decile scores equal or above 5 to be a positive recidivism prediction since the decile score goes from 1 to 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What is the accuracy of the COMPAS scores to predict recidivism**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", accuracy_score(dfr[\"two_year_recid\"], dfr[\"pred_is_recid\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Is the accuracy higher/lower if we look at particular races/genders?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "races = df[\"race\"].unique().tolist()\n",
    "genders = [\"Male\",\"Female\"]\n",
    "for race in races:\n",
    "    aux_df = dfr.loc[dfr['race'] == race]\n",
    "    acc = accuracy_score(aux_df[\"two_year_recid\"], aux_df[\"pred_is_recid\"])\n",
    "    print(\"Accuracy for {race} is {acc}\".format(race = race, acc = acc))\n",
    "\n",
    "print(\"\")\n",
    "    \n",
    "for sex in genders:\n",
    "    aux_df = dfr.loc[dfr['sex'] == sex]\n",
    "    acc = accuracy_score(aux_df[\"two_year_recid\"], aux_df[\"pred_is_recid\"])\n",
    "    print(\"Accuracy for {sex} is {acc}\".format(sex = sex, acc = acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: The accuracy seems to be lower for African-Americans than for any other race, which might indicate the existence of some bias. As for gender, both accuracies are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What about false positives and false negatives?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeCM (dfr,label):\n",
    "    cf_matrix = confusion_matrix(dfr[\"two_year_recid\"], dfr[\"pred_is_recid\"])\n",
    "\n",
    "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cf_matrix.flatten()]\n",
    "\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "              zip(group_names,group_counts,group_percentages)]\n",
    "\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n",
    "\n",
    "    ax.set_title('Confusion Matrix for ' + label);\n",
    "    ax.set_xlabel('\\nPredicted Values')\n",
    "    ax.set_ylabel('Actual Values ');\n",
    "\n",
    "    # Ticket labels - List must be in alphabetical order\n",
    "    ax.xaxis.set_ticklabels(['False','True'])\n",
    "    ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "    # Display the visualization of the Confusion Matrix.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "races = df[\"race\"].unique().tolist()\n",
    "genders = [\"Male\",\"Female\"]\n",
    "for race in races:\n",
    "    aux_df = dfr.loc[dfr['race'] == race]\n",
    "    makeCM(aux_df,race)  \n",
    "\n",
    "for sex in genders:\n",
    "    aux_df = dfr.loc[dfr['sex'] == sex]\n",
    "    makeCM(aux_df,sex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: From the confusion matrices we can see that the false positive rate is highest among African-Americans indicating that there's a bias against them. The opposite can be found for the other races. The False Negatives is then lower for African-Americans than for other races meaning that the model does a bad job at identifying people that will recomit a crime if they are not African-American."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Below we choose what features to use when training our algorithms. We have chosen to use:\n",
    "- **Sex**: Female, male\n",
    "- **Age category**: (<25, 25-45, >45 years) \n",
    "- **Charge degree**: Misdemeanors, felony\n",
    "- **Race**: African american, caucasian, hispanic and others (including asian and native american since we have too little data to analyze these races independently)\n",
    "- **Priors count**: Number of earlier crimes\n",
    "- **Juvenile mesdemeanor count**: Number of juvenile crimes, misdemeanors\n",
    "- **Juvenile felony count**: Number of juvenile crimes, felonies\n",
    "- **Juvenile other count**: Number of juvenile crimes, neither misdemanors nor felonies\n",
    "- **Two years recid**: Ground truth\n",
    "\n",
    "Features like e.g. names and dates are not being used since we do not consider them interesting for the prediction. \n",
    "\n",
    "We also make all features numerical (using the function \"OneHotEncoder\") since some algorithms for classifying can only handle numerical data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose features\n",
    "dfa = df[[\"sex\",\"age_cat\",\"race\",\"priors_count\",\"two_year_recid\",\"juv_misd_count\",\"juv_fel_count\",\"juv_other_count\",\"c_charge_degree\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge race groups \"asian\" and \"native american\" with \"other\"\n",
    "dfa.loc[(df.race == 'Asian'),'race']='Other'\n",
    "dfa.loc[(df.race == 'Native American'),'race']='Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make non-numerical features numerical\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "column_trans = make_column_transformer(\n",
    "    (OneHotEncoder(), ['sex', 'age_cat',\"c_charge_degree\",\"race\"]),\n",
    "    remainder='passthrough')\n",
    "d = column_trans.fit_transform(dfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean lables\n",
    "labels = column_trans.get_feature_names_out()\n",
    "\n",
    "def cleanLabels(label):\n",
    "    if(\"onehotencoder__\" in label):\n",
    "        return label.removeprefix(\"onehotencoder__\")\n",
    "    else:\n",
    "        return label.removeprefix(\"remainder__\")\n",
    "cleanedLabels = map(cleanLabels,labels)\n",
    "listedLabels = list(map(cleanLabels,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create new dataframe with only numerical data\n",
    "dfa_num = pd.DataFrame(data=d,columns=cleanedLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Train classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Below we randomly split our data into a training set (70% of the dataset) and a test set (30% of the dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import f1_score\n",
    "Y = dfa_num[\"two_year_recid\"]\n",
    "X = dfa_num.drop(columns=\"two_year_recid\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DisplayCM (cf_matrix, label):  \n",
    "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cf_matrix.flatten()]\n",
    "\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "              zip(group_names,group_counts,group_percentages)]\n",
    "\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n",
    "\n",
    "    ax.set_title('Confusion Matrix for ' + label);\n",
    "    ax.set_xlabel('\\nPredicted Values')\n",
    "    ax.set_ylabel('Actual Values ');\n",
    "\n",
    "    ## Ticket labels - List must be in alphabetical order\n",
    "    ax.xaxis.set_ticklabels(['False','True'])\n",
    "    ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "    ## Display the visualization of the Confusion Matrix.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SetupMatrix(cf, normalize):\n",
    "    predictions = cf.predict(X_test)\n",
    "    if normalize:\n",
    "        for p in predictions:\n",
    "            if p[0]>=0.5:\n",
    "                p[0] = 1\n",
    "            else:\n",
    "                p[0] = 0 \n",
    "    cm_data = X_test.copy() # do we change X_test by doing this? If so, we can't do this\n",
    "    cm_data[\"two_year_recid\"] = y_test\n",
    "    cm_data[\"predictions\"] = predictions # add the predictions to the table with data\n",
    "    \n",
    "    \n",
    "    races = [\"race_African-American\",\"race_Caucasian\",\"race_Hispanic\",\"race_Other\"]\n",
    "    genders = [\"sex_Male\",\"sex_Female\"]\n",
    "\n",
    "    for race in races:\n",
    "        aux_df = dfr.loc[dfr['race'] == race]\n",
    "        cm_data_filtered = cm_data.loc[cm_data[race] == 1.0]\n",
    "        cm = confusion_matrix(cm_data_filtered[\"two_year_recid\"], cm_data_filtered[\"predictions\"])\n",
    "        DisplayCM(cm, race)\n",
    "\n",
    "    for sex in genders:\n",
    "        aux_df = dfr.loc[dfr['sex'] == sex]\n",
    "        cm_data_filtered = cm_data.loc[cm_data[sex] == 1.0]\n",
    "        cm = confusion_matrix(cm_data_filtered[\"two_year_recid\"], cm_data_filtered[\"predictions\"])\n",
    "        DisplayCM(cm, sex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "clf.predict(X_test)\n",
    "clf.score(X_test,y_test) # accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cross validation for logistic regression\n",
    "scores = []\n",
    "for c in np.arange (0.01, 1, 0.05):\n",
    "    clf = LogisticRegression(random_state=0,C = c)\n",
    "    cv = cross_validate(clf, X_train, y_train, cv=10,scoring='f1_macro')\n",
    "    score = np.mean(cv[\"test_score\"])\n",
    "    scores.append([score,c])\n",
    "reg = max(scores)\n",
    "clf = LogisticRegression(random_state=0,C = reg[1] ).fit(X_train, y_train)\n",
    "log_reg_score = clf.score(X_test,y_test) # accuracy on test set\n",
    "print(\"Accuracy on test set for logistic regression: \", log_reg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrices for logistic regression\n",
    "SetupMatrix(clf,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Our Logistic Regression model has a higher False Positive rate for African-Americans when compared to the other races. However, it's an improvement from the COMPAS predictor since our False Positive rate is lower by more than 7% (at the cost of -2% for the True Positive rate). Yet, it's still higher than the other races' rates. In fact, the other races present a fairly high False Negative rate, meaning a lot of people predicted to not re-commit crimes will actually do so. Our False Negative rates are worse than the COMPAS' ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# summarize and plot importance of features\n",
    "importance = clf.coef_[0]\n",
    "plt.figure(figsize=(6, 4), dpi=(60))\n",
    "plt.bar(listedLabels[:-1], importance)\n",
    "plt.rcParams['font.size'] = '20'\n",
    "plt.xticks(rotation=90, fontsize=15)\n",
    "plt.title(\"Importance of features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: The most important feature for predicting recidivism is age. An age under 25 increases the risk of being predicted as high risk. An age over 45 lowers the risk of being predicted high risk. The second most importent features are related to earlier commited crimes. Considering race, african americans and caucasians are more likely to be predicted as high risk than hispanics and others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "svc.fit(X_train, y_train)\n",
    "svc.score(X_test,y_test)\n",
    "pred = svc.predict(X_test)\n",
    "print( \"Accuracy \" + str(svc.score(X_test,y_test)))\n",
    "f1 = f1_score(y_test, pred, average='macro')\n",
    "print(\"F1-Score \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for c in np.arange (0.01, 1, 0.05):\n",
    "    svc = make_pipeline(StandardScaler(), SVC(gamma='auto', C = c))\n",
    "    cv = cross_validate(svc, X_train, y_train, cv=10)\n",
    "    score = np.mean(cv[\"test_score\"])\n",
    "    scores.append([score,c])\n",
    "reg = max(scores)\n",
    "print(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svc = make_pipeline(StandardScaler(), SVC(gamma='auto',C = reg[1]))\n",
    "best_svc.fit(X_train, y_train)\n",
    "pred = best_svc.predict(X_test)\n",
    "print( \"Accuracy \" + str(best_svc.score(X_test,y_test)))\n",
    "f1 = f1_score(y_test, pred, average='macro')\n",
    "print(\"F1-Score \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrices for SVC\n",
    "SetupMatrix(best_svc,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Just like for the Logistic Regression, we're able to improve the False Positive rate for the African-Americans, yet this value is still higher when compared to the other races. And we can draw the same conclusion regarding the False Negative rates being too high, higher than the COMPAS' rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k_scores = []\n",
    "for k in range(3,12,2):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "    cv = cross_validate(neigh, X_train, y_train, cv=5)\n",
    "    k_score = np.mean(cv[\"test_score\"])\n",
    "    k_scores.append([k_score,k])\n",
    "k_reg = max(k_scores)\n",
    "print(k_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_neigh = KNeighborsClassifier(n_neighbors=k_reg[1])\n",
    "best_neigh.fit(X_train, y_train)\n",
    "pred = best_neigh.predict(X_test)\n",
    "print( \"Accuracy \" + str(best_neigh.score(X_test,y_test)))\n",
    "f1 = f1_score(y_test, pred, average='macro')\n",
    "print(\"F1-Score \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrices for Nearest Neighbors\n",
    "SetupMatrix(best_neigh,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: We seem to have an improvement on this classifier regarding the False Positive rates. These have a much smaller variation between races, meaning we're heading torwards a fairer model. We're also able to drop the False Negative rates slightly compared to the previous classifiers, edging closer to the COMPAS' rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "##code from https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "rf = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_random.best_params_)\n",
    "rf_random.best_params_[\"min_samples_leaf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = RandomForestClassifier(max_depth=rf_random.best_params_[\"max_depth\"], random_state=0, n_estimators=rf_random.best_params_[\"n_estimators\"],min_samples_split=rf_random.best_params_[\"min_samples_split\"],min_samples_leaf=rf_random.best_params_[\"min_samples_leaf\"],max_features=rf_random.best_params_[\"max_features\"],bootstrap=rf_random.best_params_[\"bootstrap\"])\n",
    "best_rf.fit(X_train, y_train)\n",
    "pred = best_rf.predict(X_test)\n",
    "print( \"Accuracy \" + str(best_rf.score(X_test,y_test)))\n",
    "f1 = f1_score(y_test, pred, average='macro')\n",
    "print(\"F1-Score \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrices for Random Forest\n",
    "SetupMatrix(best_rf,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: With this model, we achieve our lowest False Positive rate for African-Americans while maintaining a slightly lower rate for the other races. The False Negative rates are still within the expected values (similar to Nearest Neighbors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#https://medium.com/luca-chuangs-bapm-notes/build-a-neural-network-in-python-binary-classification-49596d7dcabf\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X.shape[1],), activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary() \n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='Adam', \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# early stopping callback\n",
    "# This callback will stop the training when there is no improvement in  \n",
    "# the validation loss for 10 consecutive epochs.  \n",
    "es = EarlyStopping(monitor='val_accuracy', \n",
    "                                   mode='max', # don't minimize the accuracy!\n",
    "                                   patience=10,\n",
    "                                   restore_best_weights=True)\n",
    "\n",
    "# now we just update our model fit call\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    callbacks=[es],\n",
    "                    epochs=500, # you can set this to a big number!\n",
    "                    batch_size=10,\n",
    "                    validation_data=(X_test,y_test),\n",
    "                    shuffle=True,\n",
    "                    verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "history_dict = history.history\n",
    "# Learning curve(Loss)\n",
    "# let's see the training and validation loss by epoch\n",
    "\n",
    "# loss\n",
    "loss_values = history_dict['loss'] # you can change this\n",
    "val_loss_values = history_dict['val_loss'] # you can also change this\n",
    "\n",
    "# range of X (no. of epochs)\n",
    "epochs = range(1, len(loss_values) + 1) \n",
    "\n",
    "# plot\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'orange', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Learning curve(accuracy)\n",
    "# let's see the training and validation accuracy by epoch\n",
    "\n",
    "# accuracy\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "# range of X (no. of epochs)\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# plot\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "# orange is for \"orange\"\n",
    "plt.plot(epochs, val_acc, 'orange', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# this is the max value - should correspond to\n",
    "# the HIGHEST train accuracy\n",
    "#np.max(val_acc)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrices for Neural Network\n",
    "SetupMatrix(model, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Our results with the Neural Network are similar to COMPAS, except with the extreme improvement on the False Positive rate, meaning we're not falsefully predicting as many people to re-commit crimes as COMPAS. So we get a strong True Positive rate and a decent False Positive, a vast improvement from COMPAS and one of the best classifiers we trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Fair classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a fair classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklego.linear_model import DemographicParityClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklego.metrics import p_percent_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "fair_classifier = DemographicParityClassifier(sensitive_cols=[\"race_African-American\",\"race_Caucasian\",\"race_Hispanic\",\"race_Other\"], covariance_threshold=0.5)\n",
    "fair_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
