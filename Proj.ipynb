{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Project\n",
    "\n",
    "## Dataset\n",
    "\n",
    "You will examine the ProPublica COMPAS dataset, which consists of all criminal defendants who were subject to COMPAS screening in Broward County, Florida, during 2013 and 2014. For each defendant, various information fields (‘features’) were also gathered by ProPublica. Broadly, these fields are related to the defendant’s demographic information (e.g., gender and race), criminal history (e.g., the number of prior offenses) and administrative information about the case (e.g., the case number, arrest date, risk of recidivism predicted by the COMPAS tool). Finally, the dataset also contains information about whether the defendant did actually recidivate or not.\n",
    "\n",
    "The COMPAS score uses answers to 137 questions to assign a risk score to defendants -- essentially a probability of re-arrest. The actual output is two-fold: a risk rating of 1-10 and a \"low\", \"medium\", or \"high\" risk label.\n",
    "\n",
    "Link to dataset: https://github.com/propublica/compas-analysis\n",
    "\n",
    "The file we will analyze is: compas-scores-two-years.csv\n",
    "\n",
    "Link to the ProPublica article:\n",
    "\n",
    "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\n",
    "\n",
    "\n",
    "## Project goal\n",
    "\n",
    "The project has three parts: \n",
    "\n",
    "- The COMPAS scores have been shown to have biases against certain racial groups. Analyze the dataset to highlight these biases.  \n",
    "\n",
    "- Based on the features in the COMPAS dataset, train classifiers to predict who will re-offend (hint: no need to use all features, just the ones you find relevant).  Study if your classifiers are more or less fair than the COMPAS classifier. \n",
    "\n",
    "- Build a fair classifier (last lecture will cover fair classification techniques). Is excluding the race from the feature set enough?\n",
    "\n",
    "\n",
    "## Today\n",
    "\n",
    "Explore the dataset and do some initial statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "\n",
    "We first need to load the data from the ProPublica repo:\n",
    "https://github.com/propublica/compas-analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "from random import seed, shuffle\n",
    "#from __future__ import division\n",
    "#from collections import defaultdict\n",
    "#import utils as ut\n",
    "\n",
    "SEED = 1234\n",
    "seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "pd.set_option('display.max_columns', None)\n",
    "def check_data_file(fname):\n",
    "    files = os.listdir(\".\") # get the current directory listing\n",
    "    print(\"Looking for file '%s' in the current directory...\",fname)\n",
    "\n",
    "    if fname not in files:\n",
    "        print(\"'%s' not found! Downloading from GitHub...\",fname)\n",
    "        addr = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
    "        response = urllib.request.urlopen(addr)\n",
    "        data = response.read()\n",
    "        fileOut = open(fname, \"wb\")\n",
    "        fileOut.write(data)\n",
    "        fileOut.close()\n",
    "        print(\"'%s' download and saved locally..\",fname)\n",
    "    else:\n",
    "        print(\"File found in current directory..\")\n",
    "    \n",
    "COMPAS_INPUT_FILE = \"compas-scores-two-years.csv\"\n",
    "check_data_file(COMPAS_INPUT_FILE)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and clean it up\n",
    "\n",
    "__Load the data__\n",
    "\n",
    "hint: data is in csv format; pandas is a python library that can read csv files\n",
    "\n",
    "you can choose to represent your data either as a DataFrame or as a dictionary\n",
    "\n",
    "- The dataset contains data on how many convicts? \n",
    "\n",
    "- What are the features the dataset contains?\n",
    "\n",
    "hint pandas: check pandas functions shape, column, head\n",
    "\n",
    "hint dictionary: check keys() function\n",
    "\n",
    "__Cleanup the data__\n",
    "\n",
    "- Are there missing values (NaN)? are there outliers?  \n",
    "\n",
    "hint pandas: check isnull function in pandas\n",
    "\n",
    "hint dictionary: implement a for and check if the variable is None\n",
    "\n",
    "- Does ProPublica mentions how to clean the data?  \n",
    "\n",
    "__What is the effect of the following function?__\n",
    "\n",
    "df = pd.read_csv(COMPAS_INPUT_FILE)\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df = df.dropna(subset=[\"days_b_screening_arrest\"]) # dropping missing vals\n",
    "\n",
    "df = df[\n",
    "    (df.days_b_screening_arrest <= 30) &  \n",
    "    (df.days_b_screening_arrest >= -30) &  \n",
    "    (df.is_recid != -1) &\n",
    "    (df.c_charge_degree != 'O') &\n",
    "    (df.score_text != 'N/A')\n",
    "]\n",
    "\n",
    "df.reset_index(inplace=True, drop=True) # renumber the rows from 0 again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(COMPAS_INPUT_FILE)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic analysis of demographics\n",
    "\n",
    "- **What are the different races present in the dataset?** \n",
    "\n",
    "- **What is the number of people by age category?**\n",
    "\n",
    "- **What is the number of people by race?**\n",
    "\n",
    "- **What is the number of people by COMPAS score (decile_score)?**\n",
    "\n",
    "- **What is the number of people by COMPAS risk category (score_text)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"age_cat\").size()\n",
    "df.groupby(\"decile_score\").size()\n",
    "df.groupby(\"score_text\").size()\n",
    "df[\"race\"].unique().tolist()\n",
    "df.groupby(\"race\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic investigations of gender and race bias in COMPAS scores\n",
    "\n",
    "*decile_score -- is the score given by the COMPAS algorithm that estimates the risk to re-offend.*\n",
    "\n",
    "*score_text -- is the level of risk: Low, Medium, High*\n",
    "\n",
    "*two_years_recid -- is the ground truth data on whether the offender recidivated or not* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What is the mean COMPAS score (decile_score) per race and gender?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby([\"race\",\"sex\"]).mean()[\"decile_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Since there are only 11 Native American people in this data set we think we should disregard them. It is the same for the Asian group, we think there are simply not enough Asian people in order to have an unbised estimator for Asian people. For this reason, we will remove the categories \"Native American\" and \"Asian\" and place these samples in the category \"Other\". For all races we can clearly see there is more data for male than there are for female so we will have to deal with this unbalance in some way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df.race == 'Asian'),'race']='Other'\n",
    "df.loc[(df.race == 'Native American'),'race']='Other'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What is the distribution (histogram) of decile_score per race and gender?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(column='decile_score', by=[\"race\",\"sex\"],figsize = (11,12));\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: As we can see there seems to be a bias towards African American people since the histogram distribution on this race is a lot more evenly distributed when compared to the other races histograms (we can observe a higher percentage of higher scores in the African American race). For the other races we can clearly see that the  histogram is a lot more squewed to lower values. This applies both for men and women."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The two_year_recid field records whether or not each person was re-arrested for a violent offense within two years, which is what COMPAS is trying to predict.*\n",
    "\n",
    "- **How many people were re-arrested?** \n",
    "\n",
    "- **Compute the recidivism (i.e., people that got re-arrested) rates by race and gender**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"two_year_recid\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "races = df[\"race\"].unique().tolist()\n",
    "\n",
    "for race in races:\n",
    "    df.loc[df['race'] == race].groupby([\"race\",\"two_year_recid\"]).size().plot(kind = \"pie\", subplots=True , title = \"Re-arrested rate by race\",autopct='%1.0f%%',label =\"\", figsize=(3,3)) \n",
    "    plt.show()\n",
    "    \n",
    "df.loc[df['sex'] == \"Male\"].groupby([\"sex\",\"two_year_recid\"]).size().plot(kind = \"pie\", subplots=True , title = \"Re-arrested rate by race\",autopct='%1.0f%%',label =\"\", figsize=(3,3)) \n",
    "plt.show()\n",
    "\n",
    "df.loc[df['sex'] == \"Female\"].groupby([\"sex\",\"two_year_recid\"]).size().plot(kind = \"pie\", subplots=True , title = \"Re-arrested rate by race\",autopct='%1.0f%%',label =\"\", figsize=(3,3)) \n",
    "plt.show()\n",
    "    \n",
    "    \n",
    "#try bar chart later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "races_names = list(df[\"race\"].unique().tolist())\n",
    "races_size = list(df.groupby(\"race\").size())\n",
    "\n",
    "plt.bar(races_names, races_size)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr = df[[\"decile_score\",\"two_year_recid\",\"sex\",\"race\"]]\n",
    "\n",
    "def is_recid(row):\n",
    "    if(row[\"decile_score\"] >= 5):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "dfr['pred_is_recid'] = dfr.apply (lambda row: is_recid(row), axis=1)\n",
    "dfr.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What is the accuracy of the COMPAS scores to predict recidivism**\n",
    "\n",
    "- **Is the accuracy higher/lower if we look at particular races/genders?**\n",
    "\n",
    "- **What about false positives and false negatives?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(dfr[\"two_year_recid\"], dfr[\"pred_is_recid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "races = df[\"race\"].unique().tolist()\n",
    "genders = [\"Male\",\"Female\"]\n",
    "for race in races:\n",
    "    aux_df = dfr.loc[dfr['race'] == race]\n",
    "    acc = accuracy_score(aux_df[\"two_year_recid\"], aux_df[\"pred_is_recid\"])\n",
    "    print(\"Acc for {race} is {acc}\".format(race = race, acc = acc))\n",
    "\n",
    "for sex in genders:\n",
    "    aux_df = dfr.loc[dfr['sex'] == sex]\n",
    "    acc = accuracy_score(aux_df[\"two_year_recid\"], aux_df[\"pred_is_recid\"])\n",
    "    print(\"Acc for {sex} is {acc}\".format(sex = sex, acc = acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeCM (dfr,label):\n",
    "    cf_matrix = confusion_matrix(dfr[\"two_year_recid\"], dfr[\"pred_is_recid\"])\n",
    "\n",
    "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cf_matrix.flatten()]\n",
    "\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "              zip(group_names,group_counts,group_percentages)]\n",
    "\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n",
    "\n",
    "    ax.set_title('Confusion Matrix for ' + label);\n",
    "    ax.set_xlabel('\\nPredicted Values')\n",
    "    ax.set_ylabel('Actual Values ');\n",
    "\n",
    "    ## Ticket labels - List must be in alphabetical order\n",
    "    ax.xaxis.set_ticklabels(['False','True'])\n",
    "    ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "    ## Display the visualization of the Confusion Matrix.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "races = df[\"race\"].unique().tolist()\n",
    "genders = [\"Male\",\"Female\"]\n",
    "for race in races:\n",
    "    aux_df = dfr.loc[dfr['race'] == race]\n",
    "    makeCM(aux_df,race)  \n",
    "\n",
    "for sex in genders:\n",
    "    aux_df = dfr.loc[dfr['sex'] == sex]\n",
    "    makeCM(aux_df,sex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: From the confusion matrices we can see that the false positive rate is highest among african americans indicating that there's a bias against them. The opposite can be found for the other races. The False Negatives is then lower for African American than for other races meaning that the model does a bad job at identifying people that will recomit a crime if they are not african american."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = df[[\"sex\",\"age_cat\",\"race\",\"priors_count\",\"two_year_recid\",\"juv_misd_count\",\"juv_fel_count\",\"juv_other_count\",\"c_charge_degree\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa.loc[(df.race == 'Asian'),'race']='Other'\n",
    "dfa.loc[(df.race == 'Native American'),'race']='Other'\n",
    "dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "column_trans = make_column_transformer(\n",
    "    (OneHotEncoder(), ['sex', 'age_cat',\"c_charge_degree\",\"race\"]),\n",
    "    remainder='passthrough')\n",
    "d = column_trans.fit_transform(dfa)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = column_trans.get_feature_names_out()\n",
    "\n",
    "def cleanLabels(label):\n",
    "    if(\"onehotencoder__\" in label):\n",
    "        return label.removeprefix(\"onehotencoder__\")\n",
    "    else:\n",
    "        return label.removeprefix(\"remainder__\")\n",
    "cleanedLabels = map(cleanLabels,labels)\n",
    "listedLabels = list(map(cleanLabels,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa_num = pd.DataFrame(data=d,columns=cleanedLabels)\n",
    "dfa_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa_num.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import f1_score\n",
    "Y = dfa_num[\"two_year_recid\"]\n",
    "X = dfa_num.drop(columns=\"two_year_recid\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "clf.predict(X_test)\n",
    "clf.score(X_test,y_test)\n",
    "#cross validation for reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for c in np.arange (0.01, 1, 0.05):\n",
    "    clf = LogisticRegression(random_state=0,C = c)\n",
    "    cv = cross_validate(clf, X_train, y_train, cv=10,scoring='f1_macro')\n",
    "    score = np.mean(cv[\"test_score\"])\n",
    "    scores.append([score,c])\n",
    "reg = max(scores)\n",
    "print(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0,C = reg[1] ).fit(X_train, y_train)\n",
    "clf.predict(X_test)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "importance = clf.coef_[0]\n",
    "# summarize feature importance\n",
    "j = 0\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0s, Score: %.5f' % (listedLabels[j],v))\n",
    "    j += 1\n",
    "# plot feature importance\n",
    "print(len(listedLabels))\n",
    "print(len(importance))\n",
    "plt.figure(figsize=(10, 5), dpi=80)\n",
    "plt.bar(listedLabels[:-1], importance)\n",
    "plt.rcParams['font.size'] = '25'\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-add asians and native americans\n",
    "#Testing from which number to split from\n",
    "#we can drop some cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "svc.fit(X_train, y_train)\n",
    "svc.score(X_test,y_test)\n",
    "pred = svc.predict(X_test)\n",
    "print( \"Accuracy \" + str(svc.score(X_test,y_test)))\n",
    "f1 = f1_score(y_test, pred, average='macro')\n",
    "print(\"F1-Score \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for c in np.arange (0.01, 1, 0.05):\n",
    "    svc = make_pipeline(StandardScaler(), SVC(gamma='auto', C = c))\n",
    "    cv = cross_validate(svc, X_train, y_train, cv=10)\n",
    "    score = np.mean(cv[\"test_score\"])\n",
    "    scores.append([score,c])\n",
    "reg = max(scores)\n",
    "print(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svc = make_pipeline(StandardScaler(), SVC(gamma='auto',C = reg[1]))\n",
    "best_svc.fit(X_train, y_train)\n",
    "pred = best_svc.predict(X_test)\n",
    "print( \"Accuracy \" + str(best_svc.score(X_test,y_test)))\n",
    "f1 = f1_score(y_test, pred, average='macro')\n",
    "print(\"F1-Score \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k_scores = []\n",
    "for k in range(3,12,2):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "    cv = cross_validate(neigh, X_train, y_train, cv=5)\n",
    "    k_score = np.mean(cv[\"test_score\"])\n",
    "    k_scores.append([k_score,k])\n",
    "k_reg = max(k_scores)\n",
    "print(k_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_neigh = KNeighborsClassifier(n_neighbors=k_reg[1])\n",
    "best_neigh.fit(X_train, y_train)\n",
    "pred = best_neigh.predict(X_test)\n",
    "print( \"Accuracy \" + str(best_neigh.score(X_test,y_test)))\n",
    "f1 = f1_score(y_test, pred, average='macro')\n",
    "print(\"F1-Score \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "##code from https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "rf = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_random.best_params_)\n",
    "rf_random.best_params_[\"min_samples_leaf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = RandomForestClassifier(max_depth=rf_random.best_params_[\"max_depth\"], random_state=0, n_estimators=rf_random.best_params_[\"n_estimators\"],min_samples_split=rf_random.best_params_[\"min_samples_split\"],min_samples_leaf=rf_random.best_params_[\"min_samples_leaf\"],max_features=rf_random.best_params_[\"max_features\"],bootstrap=rf_random.best_params_[\"bootstrap\"])\n",
    "best_rf.fit(X_train, y_train)\n",
    "pred = best_rf.predict(X_test)\n",
    "print( \"Accuracy \" + str(best_rf.score(X_test,y_test)))\n",
    "f1 = f1_score(y_test, pred, average='macro')\n",
    "print(\"F1-Score \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Index(['sex_Female', 'sex_Male', 'age_cat_25 - 45', 'age_cat_Greater than 45',\n",
    "       'age_cat_Less than 25', 'c_charge_degree_F', 'c_charge_degree_M',\n",
    "       'race_African-American', 'race_Caucasian', 'race_Hispanic',\n",
    "       'race_Other', 'priors_count', 'two_year_recid', 'juv_misd_count',\n",
    "       'juv_fel_count', 'juv_other_count'],\n",
    "      dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#https://medium.com/luca-chuangs-bapm-notes/build-a-neural-network-in-python-binary-classification-49596d7dcabf\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X.shape[1],), activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary() \n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='Adam', \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# early stopping callback\n",
    "# This callback will stop the training when there is no improvement in  \n",
    "# the validation loss for 10 consecutive epochs.  \n",
    "es = EarlyStopping(monitor='val_accuracy', \n",
    "                                   mode='max', # don't minimize the accuracy!\n",
    "                                   patience=10,\n",
    "                                   restore_best_weights=True)\n",
    "\n",
    "# now we just update our model fit call\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    callbacks=[es],\n",
    "                    epochs=500, # you can set this to a big number!\n",
    "                    batch_size=10,\n",
    "                    validation_data=(X_test,y_test),\n",
    "                    shuffle=True,\n",
    "                    verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "history_dict = history.history\n",
    "# Learning curve(Loss)\n",
    "# let's see the training and validation loss by epoch\n",
    "\n",
    "# loss\n",
    "loss_values = history_dict['loss'] # you can change this\n",
    "val_loss_values = history_dict['val_loss'] # you can also change this\n",
    "\n",
    "# range of X (no. of epochs)\n",
    "epochs = range(1, len(loss_values) + 1) \n",
    "\n",
    "# plot\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'orange', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Learning curve(accuracy)\n",
    "# let's see the training and validation accuracy by epoch\n",
    "\n",
    "# accuracy\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "# range of X (no. of epochs)\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# plot\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "# orange is for \"orange\"\n",
    "plt.plot(epochs, val_acc, 'orange', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# this is the max value - should correspond to\n",
    "# the HIGHEST train accuracy\n",
    "#np.max(val_acc)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
